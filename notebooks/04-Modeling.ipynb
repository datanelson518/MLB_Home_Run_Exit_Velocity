{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "With our data cleaned, explored and now pre-processed for modeling I can now begin to run some models to predict the home run exit velocities for each home run that was hit in the 2015, 2016 and 2017 seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error \n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Loading in the following data and pickles:\n",
    "- Scaled data as X_train_sc and X_test_sc\n",
    "- Original X_train and X_test data frames\n",
    "- Original y_train and y_test target variable arrays\n",
    "- Pickled Standard Scaler as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = pd.read_csv('../data/X_train_sc.csv', header=None)\n",
    "X_test_sc = pd.read_csv('../data/X_test_sc.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_pickle('../pickles/y_train.pkl')\n",
    "y_test = pd.read_pickle('../pickles/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_pickle('../pickles/standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Statistical Evidence\n",
    "\n",
    "Prior to beginning the modeling process I want to statistically check if there is significant evidence that launch speeds have changed from season to season between the 2015, 2016 and 2017 seasons.\n",
    "\n",
    "##### Interpretation (2015 to 2016)\n",
    "\n",
    "Using a .05 alpha (or 95% confidence interval), p=0.026 < a=.05 meaning we can reject the **H0** and conclude that there is significant evidence that the launch speeds between 2015 and 2016 is not equal.\n",
    "\n",
    "##### Interpretation (2016 to 2017)\n",
    "\n",
    "Using a .05 alpha (or 95% confidence interval), p=0.0027 < a=.05 meaning we can reject the **H0** and conclude that there is significant evidence that the launch speeds between 2016 and 2017 is not equal.\n",
    "\n",
    "\n",
    "Now, that we can see statistical evidence of differences between the seasons lets run some models to see what is influencing a batters launch speed for home run hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "- **H0**: Launch Speed 2015 = Launch Speed 2016\n",
    "- **H1**: Launch Speed 2015 != Launch Speed 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-2.2139224419171395, pvalue=0.026862949014065347)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(y_train[(X_train.game_year_2016 != 1) & (X_train.game_year_2017 != 1)], y_train[X_train.game_year_2016== 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "- **H0**: Launch Speed 2016 = Launch Speed 2017\n",
    "- **H1**: Launch Speed 2016 != Launch Speed 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.000670607720154, pvalue=0.0027015589253692584)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(y_train[X_train.game_year_2016 == 1], y_train[X_train.game_year_2017== 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model and Metrics\n",
    "\n",
    "#### Average Launch Speeds\n",
    "\n",
    "To get a baseline for the target variable (launch speed) I will take the average of the y_train and y_test variables which contain the actual launch speeds for each home run that was hit since 2015. This will give me an idea of what the average of my model predictions should be targeting.\n",
    "\n",
    "#### R2 Score and Root Mean Squared Error\n",
    "\n",
    "Lets also understand some baseline metrics in the target variable. This will then provide some insight into what my model should be targeting or performing better than:\n",
    "- **R2:** Is the explained variation in the target variable from the features provided in the model. In other words the variation in the model from the line of best fit. This will be explained as a percentage and we'll want to target a value closer to one.\n",
    "- **RMSE (Root Mean Square Error):** Is the average distance from the line of best fit. This can be interpreted in the same units as the target variable (launch speed). This value should be lower as we want the average distance from our line of best fit to be small.\n",
    "\n",
    "##### Interpretation of Baseline Model and Metrics\n",
    "\n",
    "The average launch speeds across the train and test batches are almost identical at around 103 mph so when we make the final predictions from the production model we'll get the mean and compare how close it is to 103 mph.\n",
    "\n",
    "The R2 score on both the train and test models against the baseline predictions (average launch speed) is giving a baseline variance of 0 meaning that the baseline R2 is the worst possible model possible. This means that if our model predictions are explaning any variance better than zero we are already in a better spot than the baseline. I will be optimizing for an R2 as close to one as possible.\n",
    "\n",
    "The RMSE score on both the train and test models against the baseline predictions (average launch speed) is very similar at around 4.4 and 4.3. Meaning that against the baseline predictins on average the predictions are off by 4.3 mph from the line of best fit. I will attempt to optimize my model to obtian a RMSE lower than 4.3 mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Launch Speed Train Data: 103.21479039479041\n",
      "Average Launch Speed Test Data: 103.2904052734375\n"
     ]
    }
   ],
   "source": [
    "print(f'Average Launch Speed Train Data: {y_train.mean()}')\n",
    "print(f'Average Launch Speed Test Data: {y_test.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score Train Data: 0.0\n",
      "R2 score Test Data: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'R2 score Train Data: {r2_score(y_train, [y_train.mean()] * len(y_train))}')\n",
    "print(f'R2 score Test Data: {r2_score(y_test, [y_test.mean()] * len(y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score Train Data: 4.439441224645179\n",
      "RMSE score Test Data: 4.365981019821426\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE score Train Data: {np.sqrt(mean_squared_error(y_train, [y_train.mean()] * len(y_train)))}')\n",
    "print(f'RMSE score Test Data: {np.sqrt(mean_squared_error(y_test, [y_test.mean()] * len(y_test)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Functions\n",
    "\n",
    "To ensure efficiency in the modeling process I have created a function to assist me in that process. \n",
    "\n",
    "##### Explanation of Function\n",
    "\n",
    "1. **Modeling Function:** All of my models are going to utilize grid searching in order to find the optimal parameters to use within the model for predictions so I will instantiate GridSearchCV within the function as gs. Next, the function will call upon the fit method in oder to fit the necessary data that will be used to train the model and ultimately make predictions. Upon completion of the fitting a saved pickle of the model will be saved with the key of the dictionary from the corresponding pipeline (model abbreviation) and the time stamp (in seconds) to be able to uniquely identify which model the pickle belongs.\n",
    "    - The following arguments will be required to run this function:\n",
    "        1. pipe : each of my models will have a pipeline setup prior to use with the model to use\n",
    "        2. params : each of my models will have parameters setup for the identified model in the pipeline to be used for grid searching oaver for the best parameters\n",
    "        3. X_train : the training batch of data to be used for training the model during fit\n",
    "        4. y_train : the training target varibale values to be used for training the model during fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_func(pipe, params, X_train, y_train, cv=3):\n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=cv)\n",
    "    gs.fit(X_train, y_train)\n",
    "    with open(f'../pickles/{\"_\".join(pipe.named_steps.keys())}{int(time.time())}.pkl', 'wb+') as f:\n",
    "        pickle.dump(gs, f)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models\n",
    "\n",
    "To start the modeling process I want to call upon specific models that are focused on multiple linear regression to get an understanding about if any of these modeling techniques are the optimal models to make the predictions on my data. (NOTE: From EDA I saw that less than half of my features had strong linear correlations to the target variable (launch speed) so these model may not be the correct models for this data but to confirm this I will run these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with Ridge (L2)\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### Ridge (L2)\n",
    "\n",
    "Ridge regression is a regularization technique that allows the model to take on more weight related to variance. This technique is used when we have strong multicolinearity amougnst our selected feature variables. The Ridge regression imposes a penalty on the estimates to those that were identified as performing the worst in the model by taking the sum of the square coefficients. In this technique a feature will never be zeroed out but just be brought closer and closer to zero to help the model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rd = pd.read_pickle('../pickles/rid1539795639.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameter\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rd = Pipeline([\n",
    "    ('rid', Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rd = {\n",
    "    'rid__alpha':np.logspace(-1, 3, 9)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Ridge model is scoring as follows:\n",
    "- Train R2: .515\n",
    "- Test R2: .504\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "Both my train and test scores are indicating that 51% and 50% of the variations in the target variable (launch speed) are being explained by the features in my data. This is better than the baseline R2 which means its better but the explained variance is still pretty low and is also considered over fit becasue the train score is larger than the test score. I want to find a model that will score higher in explained variance so I will move on to the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_rd = modeling_func(pipe_rd, params_rd, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rid__alpha': 31.622776601683793}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('rid', Ridge(alpha=31.622776601683793, copy_X=True, fit_intercept=True,\n",
       "   max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "   tol=0.001))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rd.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gs_rd.score(X_train_sc, y_train)\n",
    "test = gs_rd.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train Score: 0.5158479079381048\n",
      "Model Test Score: 0.5041979420168723\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Train Score: {train}')\n",
    "print(f'Model Test Score: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with Lasso (L1)\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### Lasso (L1)\n",
    "\n",
    "Lasso regression is a regularization technique that allows the model to take on more weight related to variance. This technique is used when we have strong multicolinearity amougnst our selected feature variables. The lasso regression imposes a penalty on the estimates to those that were identified as performing the worst in the model by taking the absolute value. In this technique these features will be zeroed out and not used in the model to provide better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_la = pd.read_pickle('../pickles/la1539796474.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameter\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_la = Pipeline([\n",
    "    ('la', Lasso())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_la = {\n",
    "    'la__alpha':np.logspace(-3, 3, 7)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Ridge model is scoring as follows:\n",
    "- Train R2: .514\n",
    "- Test R2: .504\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "Both my train and test scores are indicating that 51% and 50% of the variations in the target variable (launch speed) are being explained by the features in my data. This is better than the baseline R2 which means its better but the explained variance is still pretty low and is also considered over fit becasue the train score is larger than the test score. This model also scored .001% worse in explained variance than the Ridge model and in my search for a model that is better and will score higher in explained variance I will move on to the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_la = modeling_func(pipe_la, params_la, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'la__alpha': 0.01}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_la.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('la', Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_la.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5141935832528688"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_la.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5040682614801717"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_la.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with ElasticNet\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "\n",
    "#### ElasticNet\n",
    "\n",
    "The elastic net regularization technique imposes the lasso and ridge penalties (L1 and L2) on the estimates to those that were identified as performing the worst in the model. In this technique the elastic net will choose the optimal method (Lasso or Ridge) if one is found to be more optimal the the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_enet = pd.read_pickle('../pickles/enet1539795204.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_enet = Pipeline([\n",
    "    ('enet', ElasticNet())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_enet = {\n",
    "    'enet__alpha': np.logspace(-3, 3, 7),\n",
    "    'enet__l1_ratio': [.0001, .3, .5, .7, .9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Ridge model is scoring as follows:\n",
    "- Train R2: .514\n",
    "- Test R2: .504\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "Both my train and test scores are indicating that 51% and 50% of the variations in the target variable (launch speed) are being explained by the features in my data. This is better than the baseline R2 which means its better but the explained variance is still pretty low and is also considered over fit becasue the train score is larger than the test score. \n",
    "\n",
    "The intersting thing about the ElasticNet is that from the parameters I gave for the GridSearch to tune over the parameters chosen indicate that the model decided that the same parameters as the Lasso model were the most optimal meaning that my ElasticNet model will yield the same scores as the Lasso model that I ran earlier. Again, I want to get better so moving onto the next model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gs_enet = modeling_func(pipe_enet, params_enet, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enet__alpha': 0.01, 'enet__l1_ratio': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_enet.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('enet', ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=1,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_enet.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5141935832528688"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_enet.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5040682614801717"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_enet.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Models\n",
    "\n",
    "Now that we've confirmed that the multiple linear regression models are not scoring as well as I'd like lets move into ensemble modeling. Ensemble modeling will run two or models and then using the models create an accuracy score for the target variable based on the features provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with RandomForestRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### RandomForest\n",
    "\n",
    "This ensemble modeling technique will create decision trees from a random subset of features in the dataset and use averaging on those trees to improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf = pd.read_pickle('../pickles/rf1539714652.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('rf', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'rf__n_estimators':[130, 140, 150],\n",
    "    'rf__max_depth':[20, 25, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Random Forest model is scoring as follows:\n",
    "- Train Accuracy Score: .941\n",
    "- Test Accuracy Score: .603\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "As, expected with the Random Forest model we can see an extreme case of overfitting. The train score looks great and is explaining 94% of the variance from the line of best fit but the test score of 60 % explained variance which is almost 30% less than the train score is evidence that this model is way to over fit and is not the production model I want. Lets move onto the next ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_rf = modeling_func(pipe_rf, params_rf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 25, 'rf__n_estimators': 140}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('rf', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=140, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gs_rf.score(X_train, y_train)\n",
    "test = gs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train Score 0.941821592737002\n",
      "Model Test Score 0.603107782648939\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Train Score {train}')\n",
    "print(f'Model Test Score {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSeach with GradientBoostRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### Gradient Boost\n",
    "\n",
    "When boosting a model the model is building multiple simple models and learning from these models to be more approximate when predicting. These simple models are referred to as weak model or weak learners. \n",
    "\n",
    "Gradient Boosting looks at these weak models sequentially and trains on the residuals or errors in order to give more importance to the less accurate predictions and once completed uses what was learned from these predictions to combine with the strong predictions to have a better overall approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_gb = pd.read_pickle('../pickles/gbr1539715514.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gb = Pipeline([\n",
    "    ('gbr', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gb = {\n",
    "    'gbr__n_estimators':[200, 210, 220],\n",
    "    'gbr__max_depth':[3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Random Forest model is scoring as follows:\n",
    "- Train Accuracy Score: .68\n",
    "- Test Accuracy Score: .616\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "Now we are beginning to see a better model. The train score is now explaining around 68% of the variance in the predictions from the features in the dataset along with the test explaining around 61% of the variance. You can still quantify this model as over fit as the training score is greater than the testing score but the difference is not to far off and in my opinion this is the best model up to this point. Lets review one more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_gb = modeling_func(pipe_gb, params_gb, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbr__max_depth': 3, 'gbr__n_estimators': 210}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('gbr', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=210, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gs_gb.score(X_test_sc, y_test)\n",
    "train = gs_gb.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train 0.680384206966165\n",
      "Model Test 0.6165711841636452\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Train {train}')\n",
    "print(f'Model Test {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with AdaBoostRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "Again, boosting a model is the process of building multiple simple models and learning from these models to be more approximate when predicting. These simple models are referred to as weak model or weak learners.\n",
    "\n",
    "AdaBoost will work similarily to Gradient Boost in that it looks at these weak learners to train except it will modify the weights attached to the less accurate predictions and then combine what its learned back to the stronger predictions to make a better overall approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_ada = pd.read_pickle('../pickles/ada1539716634.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ada = Pipeline([\n",
    "    ('ada', AdaBoostRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ada = {\n",
    "    'ada__n_estimators':[40, 50, 60]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Random Forest model is scoring as follows:\n",
    "- Train Accuracy Score: .483\n",
    "- Test Accuracy Score: .452\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "Interesting. This model is definitely the least over fit of the ensemble models that I've run but it's scoring very low. The train score is only explaining about 48 % of the variance in the launch speed predictions against the features in the dataset with the testing score only explaining 45% of the variance.\n",
    "\n",
    "At this point I believe the Gradient Boosting Model to be the best of the models I've run and this will be my production level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_ada = modeling_func(pipe_ada, params_ada, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada__n_estimators': 50}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('ada', AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "         n_estimators=50, random_state=None))])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = gs_ada.score(X_train_sc, y_train)\n",
    "test = gs_ada.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Train Score: 0.4836915544103218\n",
      "Model Test Score: 0.452611043715773\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Train Score: {train}')\n",
    "print(f'Model Test Score: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On to the Production Level Model: 05-Production_Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
